#' Extract ROI timeseries and connectivity matrices
#'
#' Given a postprocessed BOLD NIfTI file and one or more atlas images,
#' this function computes the mean timeseries within each ROI and
#' optionally computes ROI-to-ROI correlation matrices.
#'
#' Voxels labelled in the atlas but lying outside the brain are
#' automatically excluded by intersecting with a brain mask derived from
#' the input timeseries.
#'
#' @param bold_file Path to a 4D NIfTI file containing postprocessed BOLD data.
#' @param atlas_files Character vector of atlas NIfTI files with integer
#'   ROI labels.
#' @param out_dir Directory where output files should be written.
#' @param log_file If not `NULL`, the log file to which details should be written.
#' @param cor_method Correlation method(s) to use when computing functional
#'   connectivity. Supported options include "pearson", "spearman",
#'   "kendall", and "cor.shrink". Use "none" to skip correlation
#'   computation. Multiple methods may be supplied.
#' @param roi_reduce Method used to summarize voxel time series within each
#'   ROI. Options are "mean" (default), "median", "pca", or "huber".
#' @param brain_mask Optional brain mask NIfTI file. If \code{NULL}, a mask
#'   is generated by excluding voxels with zero variance across time.
#' @param min_vox_per_roi The minimum number of voxels required for an ROI to
#'   be extracted and entered into correlations. If the ROI is smaller than this,
#'   it show up as NA in the outputs, keeping the dimensionality of the connectivity
#'   matrix consistent across inputs. Default: `5`
#' @param save_ts If `TRUE`, save the ROI time series (aggregated using `roi_reduce` method)
#'   to `_timeseries.tsv`. files. Useful for running external analyses on the ROIs. Default: `TRUE`.
#' @param rtoz If `TRUE`, using Fisher's z (aka atanh) transformation on correlations to make them
#'   continuous and unbounded, rather than `[0,1]`. The diagonal of the correlation matrices beccomes
#'   15 to approximate the 1.0 correlation, rather than making it `Inf`.
#' @param overwrite If `TRUE`, overwrite existing timeseries.tsv or connectivity.tsv files.
#' 
#' @return A named list. Each element corresponds to an atlas and contains
#'   paths to the written timeseries (\code{timeseries}) and correlation
#'   matrix (\code{correlation}, or \code{NULL} if not computed).
#' @export
extract_rois <- function(bold_file, atlas_files, out_dir, log_file = NULL,
                         cor_method = c("pearson", "spearman", "kendall", "cor.shrink"),
                         roi_reduce = c("mean", "median", "pca", "huber"),
                         brain_mask = NULL, min_vox_per_roi = 5, save_ts = TRUE, rtoz = FALSE,
                         overwrite = FALSE) {
  checkmate::assert_file_exists(bold_file)
  checkmate::assert_character(atlas_files, any.missing = FALSE, min.len = 1)
  checkmate::assert_directory_exists(out_dir, access = "w")
  cor_method <- match.arg(cor_method, several.ok = TRUE)
  roi_reduce <- match.arg(roi_reduce)
  checkmate::assert_integerish(min_vox_per_roi, len = 1L, lower = 1L)
  checkmate::assert_flag(save_ts)
  checkmate::assert_flag(rtoz)

  lg <- lgr::get_logger_glue("extract_rois")
  lg$config(NULL)
  if (!is.null(log_file)) lg$add_appender(lgr::AppenderFile$new(log_file), name = "extract_logger")

  # Read 4D NIfTI
  bold_img <- RNifti::readNifti(bold_file)
  dim_img <- dim(bold_img)
  if (length(dim_img) != 4L) stop("bold_file must be a 4D NIfTI image")

  # Flatten to a voxels x time matrix -- faster and easier
  n_time <- dim_img[4]
  mat <- matrix(bold_img, prod(dim_img[1:3]), n_time)

  # Determine brain mask
  if (is.null(brain_mask)) {
    # if no mask provided, remove zero voxels and constant. Check all(v==0) because it's faster than var()
    brain_mask_vec <- apply(mat, 1, function(v) !all(v == 0) && stats::var(v) > 0)
  } else if (checkmate::test_file_exists(brain_mask)) {
    brain_mask_vec <- as.vector(RNifti::readNifti(brain_mask)) > 0
  } else {
    to_log(lg, "fatal", "brain_mask must be a valid NIfTI file or NULL")
  }

  compute_correlation <- !is.null(cor_method) && length(cor_method) > 0L
  bids_info <- as.list(extract_bids_info(bold_file))
  sub_id <- bids_info$subject
  outputs <- list()

  # loop over atlases
  for (atlas in atlas_files) {
    checkmate::assert_file_exists(atlas)
    atlas_img <- RNifti::readNifti(atlas)

    atlas_name <- sub("\\.nii(\\.gz)?$", "", basename(atlas))
    out_dir_atlas <- file.path(out_dir, atlas_name)
    if (!dir.exists(out_dir_atlas)) dir.create(out_dir_atlas, recursive = TRUE)
    
    # expected timeseries file
    ts_bids <- modifyList(bids_info, list(rois = bids_camelcase(atlas_name), suffix = "timeseries", ext = ".tsv"))
    ts_file <- file.path(out_dir_atlas, construct_bids_filename(ts_bids, full.names = FALSE))

    # compare dimensions of atlas and image
    if (!identical(dim(atlas_img)[1:3], dim_img[1:3])) {
      to_log(lg, "fatal", "Atlas '{atlas}' spatial dimensions {paste(dim(atlas_img)[1:3], collapse = 'x')}
           do not match BOLD grid {paste(dim_img[1:3], collapse = 'x')}.
           Resample atlas or BOLD to a common grid.")
    }

    atlas_vec <- as.vector(atlas_img)
    if (!checkmate::test_integerish(atlas_vec, tol = 1e-6)) stop("Atlas ", atlas, " contains non-integer labels (outside tolerance).")
    roi_vals <- sort(unique(atlas_vec[atlas_vec > 0]))

    # ROI reduction
    ts_mat <- sapply(roi_vals, function(lbl) {
      vox <- which(atlas_vec == lbl & brain_mask_vec)
      if (length(vox) < min_vox_per_roi) {
        to_log(lg, "info", "Fewer than {min_vox_per_roi} voxels in ROI {lbl}. Dropping")
        rep(NA_real_, n_time)
      } else {
        vals <- mat[vox, , drop = FALSE]
        bad <- apply(vals, 1, function(ts) any(is.na(ts)) || all(ts == 0) || stats::var(ts) == 0)
        if (sum(!bad) < min_vox_per_roi) {
          to_log(lg, "info", "Fewer than {min_vox_per_roi} good time series in ROI {lbl}. Dropping")
          rep(NA_real_, n_time)
        } else {
          roivox <- t(vals[!bad, , drop = FALSE]) # time x voxels
          if (roi_reduce == "pca") {
            pc <- stats::prcomp(roivox, scale. = TRUE)$x[, 1]
            mn <- rowMeans(roivox) # because direction of eigenvector is arbitrary, ensure it scales positively with the mean
            if (stats::cor(pc, mn) < 0) pc <- -pc
            pc
          } else if (roi_reduce == "median") {
            apply(roivox, 1, median)
          } else if (roi_reduce == "huber") {
            apply(roivox, 1, function(x) huber(x)$mu)
          } else { # mean
            rowMeans(roivox)
          }
        }
      }
    })

    ts_df <- as.data.frame(ts_mat)
    colnames(ts_df) <- paste0("roi", roi_vals)
    ts_df$volume <- seq_len(n_time)
    ts_df <- ts_df[, c("volume", paste0("roi", roi_vals))] # volume x rois

    # apply censor file
    censor_file <- get_censor_file(bids_info)
    if (file.exists(censor_file)) {
      censor <- as.integer(readLines(censor_file))
      to_drop <- which(1L - censor == 1L) # bad timepoints are 0 in the censor file
      if (any(to_drop)) {
        to_log(lg, "info", "Dropping volumes {paste(to_drop, collapse=', ')}")
        ts_df <- ts_df[-to_drop, , drop = FALSE]
        ts_mat <- ts_mat[-to_drop, , drop = FALSE]
      }
    }

    if (isTRUE(save_ts)) {
      if (file.exists(ts_file) && isFALSE(overwrite)) {
        to_log(lg, "info", "Not overwriting existing time series file {ts_file}")
      } else {
        if (file.exists(ts_file)) {
          to_log(lg, "info", "Overwriting subject {sub_id} extracted time series: {ts_file}")
        } else {
          to_log(lg, "info", "Writing subject {sub_id} extracted time series to {ts_file}")
        }
        data.table::fwrite(ts_df, ts_file, sep = "\t")
      }
    } else {
      ts_file <- NULL
    }
    
    enough_timepoints <- TRUE
    if (nrow(ts_mat) < 20L) {
      to_log(lg, "warn", "Only {nrow(ts_mat)} timepoints in timeseries. Cannot compute valid correlations")
      enough_timepoints <- FALSE
    }

    cor_files <- NULL
    if (enough_timepoints && compute_correlation) {
      cor_files <- lapply(cor_method, function(cmeth) {
        nacols <- which(apply(ts_mat, 2, function(col) all(is.na(col))))
        ts_use <- if (length(nacols) > 0L) ts_mat[, -nacols, drop = FALSE] else ts_mat

        if (ncol(ts_use) == 0L) {
          cmat <- matrix(NA_real_, 0, 0)
        } else {
          cmat <- if (cmeth == "cor.shrink") {
            corpcor::cor.shrink(ts_use)
          } else {
            stats::cor(ts_use, method = cmeth, use = "pairwise.complete.obs")
          }

          if (isTRUE(rtoz)) {
            to_log(lg, "debug", "Applying the Fisher z transformation to correlation coefficients.")
            cmat <- atanh(cmat)
            diag(cmat) <- NA_real_ # avoid confusing Inf on diagonal since atanh(1) is Inf
          }

          if (length(nacols) > 0L) {
            full <- matrix(NA_real_, ncol(ts_mat), ncol(ts_mat))
            keep <- setdiff(seq_len(ncol(ts_mat)), nacols)
            full[keep, keep] <- cmat
            cmat <- full
          }
        }

        cor_bids <- modifyList(bids_info, list(
          rois = bids_camelcase(atlas_name), correlation = cmeth, suffix = "connectivity", ext = ".tsv"
        ))

        cor_file <- file.path(out_dir_atlas, construct_bids_filename(cor_bids, full.names = FALSE))
        to_log(lg, "info", "Writing subject {sub_id} {cmeth} correlations to {cor_file}")
        data.table::fwrite(as.data.frame(cmat), cor_file, sep = "\t")
        cor_file
      })
      names(cor_files) <- cor_method
    }

    outputs[[atlas_name]] <- list(timeseries = ts_file, correlation = cor_files)
  }

  return(outputs)
}

#' Huber M-estimator of Location and Scale
#' Borrowed from the `MASS` package to avoid dependency
#'
#' Computes a robust estimate of the mean (`mu`) and scale (`s`) of a numeric
#' vector using Huber's Proposal 2 (Huber, 1964). The estimator iteratively
#' down-weights values that are further than \eqn{k} times the median absolute
#' deviation (MAD) from the current location estimate, yielding resistance to
#' outliers.
#'
#' @param y A numeric vector of observations. Missing values (`NA`) are removed.
#' @param k Positive numeric tuning constant controlling the amount of
#'   winsorization. Larger values of \code{k} make the estimate closer to the
#'   arithmetic mean, while smaller values increase robustness.
#'   The default is \code{1.5}.
#' @param tol Numeric convergence tolerance for the iterative updates, expressed
#'   relative to the MAD. Defaults to \code{1e-6}.
#'
#' @return A list with two elements:
#'   \describe{
#'     \item{mu}{The robust location estimate (Huber M-estimator of mean).}
#'     \item{s}{The robust scale estimate, given by the MAD of the input sample.}
#'   }
#'
#' @details
#' The algorithm starts from the sample median and the MAD, then iteratively
#' updates the location by winsorizing values outside the interval
#' \eqn{[mu - k s, mu + k s]} until convergence within \code{tol}.
#' If the MAD is zero, the function stops with an error since a scale estimate
#' cannot be computed.
#'
#' @references
#' Huber, P. J. (1964). Robust Estimation of a Location Parameter.
#' \emph{Annals of Mathematical Statistics}, 35(1), 73–101.
#'
#' @examples
#' set.seed(123)
#' x <- c(rnorm(100), 10)  # outlier
#' huber(x)
#'
#' @seealso [stats::median], [stats::mad]
#'
#' @keywords internal
#' @noRd
#' @importFrom stats mad median
huber <- function(y, k = 1.5, tol = 1.0e-6) {
  y <- y[!is.na(y)]
  n <- length(y)
  mu <- median(y)
  s <- mad(y)
  if (s == 0) stop("cannot estimate scale: MAD is zero for this sample")
  repeat{
    yy <- pmin(pmax(mu - k * s, y), mu + k * s)
    mu1 <- sum(yy) / n
    if (abs(mu - mu1) < tol * s) break
    mu <- mu1
  }
  list(mu = mu, s = s)
}